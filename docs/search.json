[
  {
    "objectID": "Program/SP500_Vol_Clussters.html",
    "href": "Program/SP500_Vol_Clussters.html",
    "title": "S&P 500 Volatility Clusters",
    "section": "",
    "text": "Goal: Use the broad stock index to model and understand the index volatility levels.\nKey methodology: Use KMeans to build clustering model and then volatility regimes and then build transitional probability distribution among the regimes"
  },
  {
    "objectID": "Program/SP500_Vol_Clussters.html#project",
    "href": "Program/SP500_Vol_Clussters.html#project",
    "title": "S&P 500 Volatility Clusters",
    "section": "",
    "text": "Goal: Use the broad stock index to model and understand the index volatility levels.\nKey methodology: Use KMeans to build clustering model and then volatility regimes and then build transitional probability distribution among the regimes"
  },
  {
    "objectID": "Program/SP500_Vol_Clussters.html#import-library",
    "href": "Program/SP500_Vol_Clussters.html#import-library",
    "title": "S&P 500 Volatility Clusters",
    "section": "Import Library",
    "text": "Import Library\n\n\nCode\nimport pyprojroot\nfrom pyprojroot.here import here\nimport os\n\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\nimport ibis\nimport ibis.selectors as s\nfrom ibis import _\nibis.options.interactive = True\nibis.options.repr.interactive.max_rows = 20\n\nfrom plotnine import ggplot, geom_line, geom_path, aes, facet_wrap, labs, scale_x_continuous, theme, element_text, scale_y_continuous, scale_x_date, scale_color_manual\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler"
  },
  {
    "objectID": "Program/SP500_Vol_Clussters.html#project-file-paths",
    "href": "Program/SP500_Vol_Clussters.html#project-file-paths",
    "title": "S&P 500 Volatility Clusters",
    "section": "Project File Paths",
    "text": "Project File Paths\n\n\nCode\nbase_path = pyprojroot.find_root(pyprojroot.has_file('.here'))\noutput_dir = os.path.join(base_path, \"Data\", \"out\")"
  },
  {
    "objectID": "Program/SP500_Vol_Clussters.html#import-data",
    "href": "Program/SP500_Vol_Clussters.html#import-data",
    "title": "S&P 500 Volatility Clusters",
    "section": "Import Data",
    "text": "Import Data\nGet a list of S&P 500 price index daily\n\n\nCode\n# Define the ticker symbol for the S&P 500 index\nticker = '^GSPC'\n\n# Define the start and end dates\nstart_date = '2013-01-01'\nend_date = '2024-12-26'\n\n# Fetch the historical data\nsp500_data = yf.download(ticker, start=start_date, end=end_date, multi_level_index=False).reset_index()\n\n# Display the data\nsp500_data.head(10)\n\n\n\n\n\n\n\n\n\nDate\nClose\nHigh\nLow\nOpen\nVolume\n\n\n\n\n0\n2013-01-02\n1462.420044\n1462.430054\n1426.189941\n1426.189941\n4202600000\n\n\n1\n2013-01-03\n1459.369995\n1465.469971\n1455.530029\n1462.420044\n3829730000\n\n\n2\n2013-01-04\n1466.469971\n1467.939941\n1458.989990\n1459.369995\n3424290000\n\n\n3\n2013-01-07\n1461.890015\n1466.469971\n1456.619995\n1466.469971\n3304970000\n\n\n4\n2013-01-08\n1457.150024\n1461.890015\n1451.640015\n1461.890015\n3601600000\n\n\n5\n2013-01-09\n1461.020020\n1464.729980\n1457.150024\n1457.150024\n3674390000\n\n\n6\n2013-01-10\n1472.119995\n1472.300049\n1461.020020\n1461.020020\n4081840000\n\n\n7\n2013-01-11\n1472.050049\n1472.750000\n1467.579956\n1472.119995\n3340650000\n\n\n8\n2013-01-14\n1470.680054\n1472.050049\n1465.689941\n1472.050049\n3003010000\n\n\n9\n2013-01-15\n1472.339966\n1473.310059\n1463.760010\n1470.670044\n3135350000\n\n\n\n\n\n\n\nClean data with ibis framework. We only need to keep Date and Close columns. Since data is downloaded live, we also archive a copy of data.\nIt is totally okay to use Pandas to clean the data. It’s just a personal preference that I prefer the modernized and portable syntax of ibis framework.\n\n\nCode\n# import to duckdb backend of ibis framework\nsp500_data_ibis = ibis.memtable(data=sp500_data)\n\nsp500_data_cleaned = (\n    sp500_data_ibis.select(\"Date\", \"Close\")\n        .mutate(Date = _.Date.date(), Close = _.Close.round(digits = 2))\n)\n# export a csv copy\nsp500_data_cleaned.to_csv(path = os.path.join(output_dir, \"sp500_close.csv\")) \n\n# preview of data\nsp500_data_cleaned\n\n\n┏━━━━━━━━━━━━┳━━━━━━━━━┓\n┃ Date       ┃ Close   ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━┩\n│ date       │ float64 │\n├────────────┼─────────┤\n│ 2013-01-02 │ 1462.42 │\n│ 2013-01-03 │ 1459.37 │\n│ 2013-01-04 │ 1466.47 │\n│ 2013-01-07 │ 1461.89 │\n│ 2013-01-08 │ 1457.15 │\n│ 2013-01-09 │ 1461.02 │\n│ 2013-01-10 │ 1472.12 │\n│ 2013-01-11 │ 1472.05 │\n│ 2013-01-14 │ 1470.68 │\n│ 2013-01-15 │ 1472.34 │\n│ 2013-01-16 │ 1472.63 │\n│ 2013-01-17 │ 1480.94 │\n│ 2013-01-18 │ 1485.98 │\n│ 2013-01-22 │ 1492.56 │\n│ 2013-01-23 │ 1494.81 │\n│ 2013-01-24 │ 1494.82 │\n│ 2013-01-25 │ 1502.96 │\n│ 2013-01-28 │ 1500.18 │\n│ 2013-01-29 │ 1507.84 │\n│ 2013-01-30 │ 1501.96 │\n│ …          │       … │\n└────────────┴─────────┘"
  },
  {
    "objectID": "Program/SP500_Vol_Clussters.html#calculate-moving-average-and-volatility",
    "href": "Program/SP500_Vol_Clussters.html#calculate-moving-average-and-volatility",
    "title": "S&P 500 Volatility Clusters",
    "section": "Calculate Moving Average and Volatility",
    "text": "Calculate Moving Average and Volatility\n\n\nCode\nreturn_window = ibis.window(preceding=30, following=0, order_by=\"Date\")\n\nsp500_data_transformed = (sp500_data_cleaned.mutate(Previous_Close = _.Close.lag())\n    .mutate(Daily_Return = ((_.Close - _.Previous_Close)/_.Previous_Close).round(digits = 6))\n    .mutate(thirty_day_vol = ibis.ifelse(\n        _.Daily_Return.count().over(return_window) &gt;= 30, \n        _.Daily_Return.std().over(return_window).round(digits = 6), \n        None))\n)\n\nsp500_vol_no_null = sp500_data_transformed.filter(_.thirty_day_vol != None)\nsp500_vol_dates = sp500_vol_no_null.select(\"Date\").mutate(index = ibis.row_number())\nsp500_vol = sp500_vol_no_null.select(\"thirty_day_vol\")\n\n# bring to pandas dataframes to be more compatible with sklearn APIs\nsp500_vol_pd = sp500_vol.to_pandas()"
  },
  {
    "objectID": "Program/SP500_Vol_Clussters.html#train-kmeans-model",
    "href": "Program/SP500_Vol_Clussters.html#train-kmeans-model",
    "title": "S&P 500 Volatility Clusters",
    "section": "Train KMeans Model",
    "text": "Train KMeans Model\n\nFind the Optimal K, using elbow method\nIn the following “elbow charts”, trade off between inertia and silhouette scores, we settle at 9 clusters, as it gives a relatively high silhouette scores whil keeping a relatively low inertia.\n\n\nCode\ndef find_best_k_for_kmeans_clustering(dataframe, scaler, kmin, kmax,  random_state=42, figheight=8, figwidth=10):\n    scalermethod = scaler;\n    dataframe_scaled = pd.DataFrame(data=scaler.fit_transform(dataframe[dataframe.columns]), columns=dataframe.columns);\n    from sklearn.metrics import silhouette_score, silhouette_samples;\n    inertias = {}\n    silhouettes = {}\n    for k in range(kmin, kmax):\n        kmeans = KMeans(n_clusters=k, random_state=random_state).fit(dataframe_scaled)\n        inertias[k] = kmeans.inertia_\n        silhouettes[k] = silhouette_score(dataframe_scaled, kmeans.labels_, metric='euclidean')\n    inertias_df = ibis.memtable(list(inertias.items()), columns=[\"cluster\", \"inertia\"])\n    silhouettes_df = ibis.memtable(list(silhouettes.items()), columns=[\"cluster\", \"silhouettes\"])\n    metrics_df = inertias_df.left_join(silhouettes_df, \"cluster\").select(~s.contains(\"_right\"))\n    return metrics_df\n\nkmeans_metrics_df = find_best_k_for_kmeans_clustering(sp500_vol_pd, scaler=MinMaxScaler(), kmin=3, kmax=15)\n\nkmeans_metrics_df\n\n\n┏━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━┓\n┃ cluster ┃ inertia  ┃ silhouettes ┃\n┡━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━┩\n│ int64   │ float64  │ float64     │\n├─────────┼──────────┼─────────────┤\n│       3 │ 5.694822 │    0.651185 │\n│       4 │ 4.690729 │    0.648749 │\n│       5 │ 2.682729 │    0.540790 │\n│       6 │ 1.579715 │    0.553520 │\n│       7 │ 1.086568 │    0.571062 │\n│       8 │ 0.837567 │    0.578115 │\n│       9 │ 0.681040 │    0.581327 │\n│      10 │ 0.605977 │    0.576432 │\n│      11 │ 0.470595 │    0.577266 │\n│      12 │ 0.401481 │    0.536613 │\n│      13 │ 0.325140 │    0.533363 │\n│      14 │ 0.263371 │    0.545676 │\n└─────────┴──────────┴─────────────┘\n\n\n\nCreate “elbow chart” for inertia (sum of squared distances within each cluster).\nWe are looking for the number of clusters that yield the relatively lower “turning” of the line.\n\n\nCode\n(\n    ggplot(kmeans_metrics_df, aes(\"cluster\", \"inertia\"))\n    + geom_line()\n    + scale_x_continuous(breaks=range(3,15))\n    + labs(\n        x=\"Number of clusters, K\",\n        y=\"Inertia\",\n        title=\"K-Means, Elbow Method\")\n)\n\n\n\n\n\n\n\n\n\nCreate “elbow chart” for silhouette scores (measuring separation among clusters).\nWe are looking for the number of clusters that yield the relatively higher “turning” of the line.\n\n\nCode\n(\n    ggplot()\n    + geom_line(kmeans_metrics_df, aes(\"cluster\", \"silhouettes\"))\n    + scale_x_continuous(breaks=range(3,15))\n    + labs(\n        x=\"Number of clusters, K\",\n        y=\"silhouettes\",\n        title=\"K-Means, Elbow Method\")\n)\n\n\n\n\n\n\n\n\n\nBased on the outputs and criteria, it seems 8 clusters are appropriate.\n\n\nTrain the Optimal KMeans Model and Cluster Volatility\nTrain the optimal KMeans model and predict the cluster label and then cluster volatilities into 9 different clusters.\n\n\nCode\ndef predict_cluster_with_kmeans(dataframe, optimal_k, scaler, Random_State=42):\n    from sklearn.cluster import KMeans;\n    kmeans = KMeans(n_clusters=optimal_k, random_state=Random_State);\n    dataframe_scaled = pd.DataFrame(data=scaler.fit_transform(dataframe[dataframe.columns]), columns=dataframe.columns);\n    dataframe['cluster'] = kmeans.fit_predict(dataframe_scaled)\n    return dataframe\n\nsp500_vol_pred = predict_cluster_with_kmeans(dataframe=sp500_vol_pd, optimal_k=8, scaler=MinMaxScaler())\n\nsp500_vol_pred.head(10)\n\n\n\n\n\n\n\n\n\nthirty_day_vol\ncluster\n\n\n\n\n0\n0.004513\n0\n\n\n1\n0.004457\n0\n\n\n2\n0.004543\n0\n\n\n3\n0.005142\n0\n\n\n4\n0.005258\n0\n\n\n5\n0.005384\n6\n\n\n6\n0.006422\n6\n\n\n7\n0.006374\n6\n\n\n8\n0.006736\n6\n\n\n9\n0.006735\n6\n\n\n\n\n\n\n\nView the descriptive stats on each cluster.\n\n\nCode\nsp500_vol_pred = (\n    ibis.memtable(data=sp500_vol_pred)\n    # adding the dates back to the volatility dataset\n        .mutate(index = ibis.row_number())\n        .left_join(sp500_vol_dates, \"index\")\n        .select(~s.startswith(\"index\"))\n)\n\n(\n    sp500_vol_pred.aggregate(\n            by = \"cluster\",\n            count = _.thirty_day_vol.count(), \n            mean = _.thirty_day_vol.mean(),\n            max =  _.thirty_day_vol.max(),\n            median = _.thirty_day_vol.median(), \n            min = _.thirty_day_vol.min())\n        .order_by(_.cluster)\n)\n\n\n┏━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n┃ cluster ┃ count ┃ mean     ┃ max      ┃ median   ┃ min      ┃\n┡━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n│ int32   │ int64 │ float64  │ float64  │ float64  │ float64  │\n├─────────┼───────┼──────────┼──────────┼──────────┼──────────┤\n│       0 │   517 │ 0.004371 │ 0.005327 │ 0.004471 │ 0.002251 │\n│       1 │   349 │ 0.013144 │ 0.015148 │ 0.013046 │ 0.011789 │\n│       2 │    31 │ 0.048867 │ 0.052927 │ 0.050229 │ 0.041277 │\n│       3 │   704 │ 0.007931 │ 0.009160 │ 0.007856 │ 0.007118 │\n│       4 │    14 │ 0.029730 │ 0.035864 │ 0.030857 │ 0.023744 │\n│       5 │   229 │ 0.017168 │ 0.023409 │ 0.016761 │ 0.015194 │\n│       6 │   732 │ 0.006277 │ 0.007115 │ 0.006308 │ 0.005332 │\n│       7 │   410 │ 0.010369 │ 0.011763 │ 0.010353 │ 0.009173 │\n└─────────┴───────┴──────────┴──────────┴──────────┴──────────┘\n\n\n\nCreate the bridging table to relabel clusters.\nRelabel the cluster in ascending order of mean volatility of each regime\nThis is used to create the transitional probability table later.\n\n\nCode\nsp500_vol_pred_cluster_bridge = (\n    sp500_vol_pred.aggregate(\n        by = _.cluster, \n        cluster_mean_vol = _.thirty_day_vol.mean())\n        .order_by(_.cluster_mean_vol)\n        .mutate(cluster_new = ibis.row_number())\n)\n\nsp500_vol_pred_relabelled = (\n    sp500_vol_pred.left_join(sp500_vol_pred_cluster_bridge, _.cluster == sp500_vol_pred_cluster_bridge.cluster)\n        .drop([\"cluster\", \"cluster_right\"])\n        .rename(cluster = \"cluster_new\")\n)\n\nsp500_vol_pred_relabelled\n\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n┃ thirty_day_vol ┃ Date       ┃ cluster_mean_vol ┃ cluster ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n│ float64        │ date       │ float64          │ int64   │\n├────────────────┼────────────┼──────────────────┼─────────┤\n│       0.004513 │ 2013-02-14 │         0.004371 │       0 │\n│       0.004457 │ 2013-02-15 │         0.004371 │       0 │\n│       0.004543 │ 2013-02-19 │         0.004371 │       0 │\n│       0.005142 │ 2013-02-20 │         0.004371 │       0 │\n│       0.005258 │ 2013-02-21 │         0.004371 │       0 │\n│       0.005384 │ 2013-02-22 │         0.006277 │       1 │\n│       0.006422 │ 2013-02-25 │         0.006277 │       1 │\n│       0.006374 │ 2013-02-26 │         0.006277 │       1 │\n│       0.006736 │ 2013-02-27 │         0.006277 │       1 │\n│       0.006735 │ 2013-02-28 │         0.006277 │       1 │\n│       0.006740 │ 2013-03-01 │         0.006277 │       1 │\n│       0.006768 │ 2013-03-04 │         0.006277 │       1 │\n│       0.006891 │ 2013-03-05 │         0.006277 │       1 │\n│       0.006879 │ 2013-03-06 │         0.006277 │       1 │\n│       0.006855 │ 2013-03-07 │         0.006277 │       1 │\n│       0.006881 │ 2013-03-08 │         0.006277 │       1 │\n│       0.006887 │ 2013-03-11 │         0.006277 │       1 │\n│       0.006874 │ 2013-03-12 │         0.006277 │       1 │\n│       0.006853 │ 2013-03-13 │         0.006277 │       1 │\n│       0.006894 │ 2013-03-18 │         0.006277 │       1 │\n│              … │ …          │                … │       … │\n└────────────────┴────────────┴──────────────────┴─────────┘\n\n\n\nReview the key volatility stats based on final clusters:\n\n\nCode\nsp500_vol_pred_relabelled_summary = (\n    sp500_vol_pred_relabelled.aggregate(\n            by = \"cluster\",\n            count = _.thirty_day_vol.count(), \n            mean = _.thirty_day_vol.mean(),\n            max =  _.thirty_day_vol.max(),\n            median = _.thirty_day_vol.median(), \n            min = _.thirty_day_vol.min())\n        .order_by(_.cluster)\n)\n\nsp500_vol_pred_relabelled_summary\n\n\n┏━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n┃ cluster ┃ count ┃ mean     ┃ max      ┃ median   ┃ min      ┃\n┡━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n│ int64   │ int64 │ float64  │ float64  │ float64  │ float64  │\n├─────────┼───────┼──────────┼──────────┼──────────┼──────────┤\n│       0 │   517 │ 0.004371 │ 0.005327 │ 0.004471 │ 0.002251 │\n│       1 │   732 │ 0.006277 │ 0.007115 │ 0.006308 │ 0.005332 │\n│       2 │   704 │ 0.007931 │ 0.009160 │ 0.007856 │ 0.007118 │\n│       3 │   410 │ 0.010369 │ 0.011763 │ 0.010353 │ 0.009173 │\n│       4 │   349 │ 0.013144 │ 0.015148 │ 0.013046 │ 0.011789 │\n│       5 │   229 │ 0.017168 │ 0.023409 │ 0.016761 │ 0.015194 │\n│       6 │    14 │ 0.029730 │ 0.035864 │ 0.030857 │ 0.023744 │\n│       7 │    31 │ 0.048867 │ 0.052927 │ 0.050229 │ 0.041277 │\n└─────────┴───────┴──────────┴──────────┴──────────┴──────────┘\n\n\n\n\n\nPlot the Regime vs Actual thirty_day_Vol\nIn the following graph, you can see how actual 30-day volatility compares to (shaded in red) their major volatility cluster.\n\n\nCode\nsp500_vol_pred_pivoted = (\n    sp500_vol_pred_relabelled.pivot_longer(col=[\"thirty_day_vol\", \"cluster_mean_vol\"], names_to=\"Type\", values_to=\"Volatility\")\n    .order_by(_.Type, _.Date)\n)\n\n(\n    ggplot(sp500_vol_pred_pivoted, aes(x = \"Date\"))\n    + geom_line(aes(y = \"Volatility\", color = \"Type\", group = 1), stat = \"identity\")\n    + scale_color_manual(values=dict(thirty_day_vol = \"blue\", cluster_mean_vol = \"red\"))\n    + scale_x_date(date_breaks=\"3 month\", date_labels=\"%Y-%m\")\n    + labs(\n        x = \"Date\", \n        y = \"Volatility\",\n        title=\"S&P500 Volatility Regime Transitions\" )\n    + theme( \n        figure_size=(20, 10),\n        legend_position='top',\n        axis_text_x=element_text(rotation=45, hjust=1) )\n)"
  },
  {
    "objectID": "Program/SP500_Vol_Clussters.html#calculate-transition-probability",
    "href": "Program/SP500_Vol_Clussters.html#calculate-transition-probability",
    "title": "S&P 500 Volatility Clusters",
    "section": "Calculate Transition Probability",
    "text": "Calculate Transition Probability\ncreate cluster transitions counts dataframe\n\n\nCode\nsp500_vol_pred_shifted = (\n    sp500_vol_pred.select(cluster_from = \"cluster\")\n        .mutate(\n            cluster_from = _.cluster_from.cast(\"String\"),\n            cluster_to = _.cluster_from.lead(1).cast(\"String\"))\n        # remove NAs from lead()\n        .filter(_.cluster_to != None)\n)\nsp500_vol_pred_shifted\n\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ cluster_from ┃ cluster_to ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ string       │ string     │\n├──────────────┼────────────┤\n│ 0            │ 0          │\n│ 0            │ 0          │\n│ 0            │ 0          │\n│ 0            │ 0          │\n│ 0            │ 6          │\n│ 6            │ 6          │\n│ 6            │ 6          │\n│ 6            │ 6          │\n│ 6            │ 6          │\n│ 6            │ 6          │\n│ 6            │ 6          │\n│ 6            │ 6          │\n│ 6            │ 6          │\n│ 6            │ 6          │\n│ 6            │ 6          │\n│ 6            │ 6          │\n│ 6            │ 6          │\n│ 6            │ 6          │\n│ 6            │ 6          │\n│ 6            │ 6          │\n│ …            │ …          │\n└──────────────┴────────────┘\n\n\n\n\n\nCode\n# get a frequecy of each cluster to calculate relative frequency of each transition type, see below\nsp500_cluster_vol_count = (\n    sp500_vol_pred.aggregate(\n                by = \"cluster\",\n                count = _.thirty_day_vol.count())\n    .mutate(cluster = _.cluster.cast(\"String\"))\n)\n\nvol_transition_table = (\n        # perform crosstabbing between cluster_from and cluster_to to understand count of transitions \n    sp500_vol_pred_shifted\n        .mutate(counter = 1)\n        .pivot_wider(names_from =\"cluster_to\", values_from=\"counter\", values_agg=\"sum\", values_fill=0, names_sort=True)\n        # add cluster frequency\n        .left_join(\n            right = sp500_cluster_vol_count, \n            predicates= _.cluster_from == sp500_cluster_vol_count.cluster\n        )\n        # clean up data\n        .drop(_.cluster_from)\n        .order_by(_.cluster)\n        .relocate(_.cluster, before=\"0\")\n)\nvol_transition_table\n\n\n┏━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃ cluster ┃ 0     ┃ 1     ┃ 2     ┃ 3     ┃ 4     ┃ 5     ┃ 6     ┃ 7     ┃ count ┃\n┡━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│ string  │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │\n├─────────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┤\n│ 0       │   466 │     5 │     0 │     8 │     0 │     2 │    32 │     4 │   517 │\n│ 1       │     2 │   300 │     0 │     4 │     0 │    15 │     5 │    23 │   349 │\n│ 2       │     0 │     0 │    29 │     0 │     1 │     1 │     0 │     0 │    31 │\n│ 3       │    11 │     5 │     0 │   586 │     1 │     1 │    73 │    26 │   704 │\n│ 4       │     0 │     0 │     1 │     0 │    11 │     1 │     0 │     1 │    14 │\n│ 5       │     0 │    14 │     1 │     4 │     1 │   203 │     1 │     5 │   229 │\n│ 6       │    35 │     1 │     0 │    70 │     0 │     3 │   614 │     9 │   732 │\n│ 7       │     2 │    24 │     0 │    32 │     0 │     3 │     7 │   342 │   410 │\n└─────────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘\n\n\n\n\n\nCode\n# calculate relative frequencies\nvol_transition_freq_table = vol_transition_table.mutate(s.across(s.numeric(), func = _ / vol_transition_table['count'])).drop(\"count\")\n\nvol_transition_freq_table\n\n\n┏━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n┃ cluster ┃ 0        ┃ 1        ┃ 2        ┃ 3        ┃ 4        ┃ 5        ┃ 6        ┃ 7        ┃\n┡━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n│ string  │ float64  │ float64  │ float64  │ float64  │ float64  │ float64  │ float64  │ float64  │\n├─────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n│ 0       │ 0.901354 │ 0.009671 │ 0.000000 │ 0.015474 │ 0.000000 │ 0.003868 │ 0.061896 │ 0.007737 │\n│ 1       │ 0.005731 │ 0.859599 │ 0.000000 │ 0.011461 │ 0.000000 │ 0.042980 │ 0.014327 │ 0.065903 │\n│ 2       │ 0.000000 │ 0.000000 │ 0.935484 │ 0.000000 │ 0.032258 │ 0.032258 │ 0.000000 │ 0.000000 │\n│ 3       │ 0.015625 │ 0.007102 │ 0.000000 │ 0.832386 │ 0.001420 │ 0.001420 │ 0.103693 │ 0.036932 │\n│ 4       │ 0.000000 │ 0.000000 │ 0.071429 │ 0.000000 │ 0.785714 │ 0.071429 │ 0.000000 │ 0.071429 │\n│ 5       │ 0.000000 │ 0.061135 │ 0.004367 │ 0.017467 │ 0.004367 │ 0.886463 │ 0.004367 │ 0.021834 │\n│ 6       │ 0.047814 │ 0.001366 │ 0.000000 │ 0.095628 │ 0.000000 │ 0.004098 │ 0.838798 │ 0.012295 │\n│ 7       │ 0.004878 │ 0.058537 │ 0.000000 │ 0.078049 │ 0.000000 │ 0.007317 │ 0.017073 │ 0.834146 │\n└─────────┴──────────┴──────────┴──────────┴──────────┴──────────┴──────────┴──────────┴──────────┘"
  }
]